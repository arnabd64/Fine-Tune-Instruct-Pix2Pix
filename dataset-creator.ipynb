{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to prepare the dataset\n",
    "\n",
    "In this notebook, I will demonstrate how to prepare the dataset to finetune Instruct Pix2Pix Stable Diffusion model. For doing so, you will be needed a free [huggingface](https://hf.co) account to store the dataset. Also note that you don't need an Amazon AWS account or any other cloud services for this notebook to run, everything here is executed on my local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "You will be needing the following python packages:\n",
    "\n",
    "- `huggingface-hub`\n",
    "- `datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --no-cache-dir huggingface-hub datasets pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain a Read/Write or Full Access Access Token by following the steps:\n",
    "\n",
    "- Login to your huggingface account.\n",
    "- Go to Settings -> Access Tokens -> New Token\n",
    "- Click on New Token. Provde a Name for your key and select the Type as Write for Full Access.\n",
    "- Copy the Key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! huggingface-cli login --token=<paste-your-token>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize the Images\n",
    "\n",
    "- Store all your **original** images in a directory named `original`.\n",
    "- Store all your **edited** images in a directory named `edited`.\n",
    "- All prompts should be stored in a single text file where each line corresponds to their respective image.\n",
    "- `original/image_1.jpg` should correspond to `edited/image_1.jpg` and so on.\n",
    "- Accordingly the prompt in line 1 of `prompts.txt` should be the prompt used for training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to directories\n",
    "from pathlib import Path\n",
    "ORIGINAL_IMAGES = Path(\"original\")\n",
    "EDITED_IMAGES = Path(\"edited\")\n",
    "PROMPTS = Path(\"prompts.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if directories exists\n",
    "if not ORIGINAL_IMAGES.exists():\n",
    "    raise FileNotFoundError(f\"Directory: {ORIGINAL_IMAGES.absolute()} not found\")\n",
    "if not EDITED_IMAGES.exists():\n",
    "    raise FileNotFoundError(f\"Directory: {EDITED_IMAGES.absolute()} not found\")\n",
    "if not PROMPTS.exists():\n",
    "    raise FileNotFoundError(f\"File: {PROMPTS.absolute()} not found\")\n",
    "\n",
    "# check if directory contains images\n",
    "ORIGINAL_IMAGES_COUNT = len(list(ORIGINAL_IMAGES.iterdir()))\n",
    "EDITED_IMAGES_COUNT = len(list(EDITED_IMAGES.iterdir()))\n",
    "\n",
    "if ORIGINAL_IMAGES_COUNT == 0:\n",
    "    raise FileNotFoundError(f\"Directory: {ORIGINAL_IMAGES.absolute()} does not contain any images\")\n",
    "else:\n",
    "    print(f\"original images: {ORIGINAL_IMAGES_COUNT}\")\n",
    "    \n",
    "if EDITED_IMAGES_COUNT == 0:\n",
    "    raise FileNotFoundError(f\"Directory: {ORIGINAL_IMAGES.absolute()} does not contain any images\")\n",
    "else:\n",
    "    print(f\"edited images: {EDITED_IMAGES_COUNT}\")\n",
    "    \n",
    "if not (ORIGINAL_IMAGES_COUNT == EDITED_IMAGES_COUNT):\n",
    "    raise ValueError(\"Mismatch in the number of images in original and edited images\")\n",
    "    \n",
    "# check if prompts.txt is empty\n",
    "with open(PROMPTS, \"r\") as fp:\n",
    "    prompts = fp.readlines()\n",
    "\n",
    "if len(prompts) == 0:\n",
    "    raise ValueError(f\"File: {PROMPTS.absolute()} does not contain any prompts\")\n",
    "elif not (len(prompts) == EDITED_IMAGES_COUNT):\n",
    "    raise ValueError(\"The number of Images don't match with the number of prompts\")\n",
    "else:\n",
    "    print(f\"Prompts: {len(prompts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_samples(original_images_path: list[Path], edited_images_path: list[Path], prompts_list: list[str]):\n",
    "    original_images: list[Image.Image] = []\n",
    "    edited_images: list[Image.Image] = []\n",
    "    \n",
    "    for orig_img, edit_img in zip(original_images_path, edited_images_path):     \n",
    "        # load images\n",
    "        original_images.append(Image.open(orig_img.absolute()))\n",
    "        edited_images.append(Image.open(edit_img.absolute()))\n",
    "        \n",
    "        # format the dataset\n",
    "        dataset_json = {\n",
    "            \"before\": original_images,\n",
    "            \"after\": edited_images,\n",
    "            \"prompt\": prompts_list\n",
    "        }\n",
    "        \n",
    "        # build the dataset\n",
    "        features = datasets.Features({\n",
    "            \"before\": datasets.Image(),\n",
    "            \"after\": datasets.Image(),\n",
    "            \"prompt\": datasets.Value('string')\n",
    "        })\n",
    "        \n",
    "    return datasets.Dataset.from_dict(dataset_json, features)\n",
    "\n",
    "\n",
    "ip2p_dataset = load_samples(ORIGINAL_IMAGES.iterdir(), EDITED_IMAGES.iterdir(), prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Dataset to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ID = \"arnabdhar/instruct-pix2pix-dataset\"\n",
    "\n",
    "ip2p_dataset.push_to_hub(\n",
    "    repo_id = REPO_ID,\n",
    "    split = 'train',\n",
    "    private = True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
